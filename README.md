# Text Summarization Project

A comprehensive comparison and analysis of different text summarization algorithms including BERT, T5, and TextRank. This project evaluates the performance of extractive and abstractive summarization techniques on various datasets.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Algorithms](#algorithms)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Datasets](#datasets)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This project implements and compares three popular text summarization algorithms:

- **BERT (Bidirectional Encoder Representations from Transformers)**: Extractive summarization using transformer-based embeddings
- **T5 (Text-to-Text Transfer Transformer)**: Abstractive summarization using sequence-to-sequence models
- **TextRank**: Graph-based extractive summarization using PageRank algorithm

The project includes both summary generation and headline generation capabilities, with comprehensive evaluation metrics and analysis.

## ğŸ¤– Algorithms

### 1. BERT Summarization
- **Type**: Extractive
- **Approach**: Uses BERT embeddings to identify and extract the most important sentences
- **Advantages**: Contextual understanding, high accuracy
- **Use Case**: Document summarization, content extraction

### 2. T5 Summarization
- **Type**: Abstractive
- **Approach**: Sequence-to-sequence model that generates new text
- **Advantages**: Can create novel summaries, better coherence
- **Use Case**: Creative summarization, headline generation

### 3. TextRank
- **Type**: Extractive
- **Approach**: Graph-based algorithm using sentence similarity and PageRank
- **Advantages**: No training required, interpretable
- **Use Case**: Quick summarization, keyword extraction

## ğŸ“ Project Structure

```
text-summarization/
â”œâ”€â”€ code/                    # Core algorithm implementations
â”‚   â”œâ”€â”€ bert.py             # BERT-based summarization
â”‚   â”œâ”€â”€ T5.py               # T5-based summarization
â”‚   â””â”€â”€ TextRank.py         # TextRank algorithm
â”œâ”€â”€ datasets/               # Data files and download scripts
â”‚   â”œâ”€â”€ business_data.csv   # Business articles dataset
â”‚   â”œâ”€â”€ train.csv          # Training data
â”‚   â”œâ”€â”€ test.csv           # Test data
â”‚   â”œâ”€â”€ validation.csv     # Validation data
â”‚   â””â”€â”€ download-dataset.py # Dataset download script
â”œâ”€â”€ summary/               # Summary generation scripts
â”‚   â”œâ”€â”€ bert_summary.py    # BERT summary evaluation
â”‚   â”œâ”€â”€ openAI_headline.py # OpenAI headline generation
â”‚   â””â”€â”€ output/            # Summary results and metrics
â”œâ”€â”€ headlines/             # Headline generation scripts
â”‚   â”œâ”€â”€ bert_headline.py   # BERT headline generation
â”‚   â””â”€â”€ output/            # Headline results and metrics
â””â”€â”€ README.md              # Project documentation
```

## ğŸš€ Installation

### Prerequisites
- Python 3.7+
- pip package manager

### Dependencies

Install the required packages:

```bash
pip install transformers
pip install torch
pip install sentence-transformers
pip install summarizer
pip install nltk
pip install gensim
pip install networkx
pip install scipy
pip install numpy
pip install pandas
```

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd text-summarization
```

2. Download required NLTK data:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

## ğŸ’» Usage

### Basic Usage

#### BERT Summarization
```python
from code.bert import Summarizer

summarizer = Summarizer()
summary = summarizer(input_text, min_length=50, max_length=1500)
```

#### T5 Summarization
```python
from code.T5 import summarize_text

summary = summarize_text(input_text, max_length=1000, min_length=500)
```

#### TextRank Summarization
```python
# Run the TextRank script directly
python code/TextRank.py
```

### Advanced Usage

#### Running Summary Evaluation
```bash
cd summary
python bert_summary.py
```

#### Running Headline Generation
```bash
cd headlines
python bert_headline.py
```

## ğŸ“Š Datasets

The project uses several datasets for evaluation:

- **Business Data**: Articles from business domain
- **Training Data**: Large-scale training dataset (1.2GB)
- **Test Data**: Evaluation dataset (48MB)
- **Validation Data**: Validation dataset (55MB)

### Downloading Datasets

Use the provided download script:
```bash
cd datasets
python download-dataset.py
```

## ğŸ“ˆ Results

The project generates comprehensive evaluation metrics including:

- **ROUGE Scores**: ROUGE-1, ROUGE-2, ROUGE-L
- **BLEU Scores**: Bilingual Evaluation Understudy
- **METEOR Scores**: Metric for Evaluation of Translation with Explicit ORdering
- **BERTScore**: Contextual embeddings-based evaluation

Results are stored in the respective `output/` directories with detailed analysis and comparisons.

## ğŸ”§ Customization

### Adjusting Summary Length
```python
# For BERT
summary = summarizer(text, min_length=100, max_length=500)

# For T5
summary = summarize_text(text, max_length=800, min_length=200)
```

### Model Selection
```python
# For T5, choose different model sizes
model_name = "t5-small"    # Fast, smaller
model_name = "t5-base"     # Balanced
model_name = "t5-large"    # High quality, slower
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Hugging Face for the transformers library
- Google Research for T5 model
- Stanford NLP for TextRank algorithm
- The summarizer library contributors

---

**Note**: This project is for educational and research purposes. Please ensure you have the necessary permissions when using copyrighted content for summarization.